import nltk
nltk.download('wordnet')

pip install pandas scikit-learn nltk

import pandas as pd

true_df = pd.read_csv('')            //location
fake_df = pd.read_csv('')            //location

true_df['label'] = 0
fake_df['label'] = 1
true_df.head()
fake_df.head()

true_df = true_df[['text','label']]
fake_df = fake_df[['text','label']]
dataset = pd.concat([true_df , fake_df])
dataset.isnull().sum()
dataset['label'].value_counts()
dataset = dataset.sample(frac = 1)
dataset.head(20)

import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
stopwords = stopwords.words('english')

def clean_data(text):
    text = text.lower() 
    text = re.sub('[^a-zA-Z]' , ' ' , text)
    token = text.split() 
    token = [lemmatizer.lemmatize(word) for word in token if not word in stopwords]  
    clean_text = ' '.join(token) 
    
    return clean_text

try:
    dataset['text'] = dataset['text'].apply(lambda x : clean_data(x))
    print("Data cleaning completed successfully.")
except Exception as e:
    print("Error occurred during data cleaning:", e)

true_samples = dataset[dataset['label'] == 0].sample(n=3000, random_state=0)
fake_samples = dataset[dataset['label'] == 1].sample(n=3000, random_state=0)
balanced_dataset = pd.concat([true_samples, fake_samples])

from sklearn.model_selection import train_test_split
X = balanced_dataset['text']
y = balanced_dataset['label']
train_X , test_X , train_y , test_y = train_test_split(X , y , test_size = 0.2 ,random_state = 42)

X.head()
y.head()
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features = 19 , lowercase=True , ngram_range=(1,2))

X = dataset.iloc[:35000,0]
y = dataset.iloc[:35000,1]
vec_train = vectorizer.fit_transform(train_X)
vec_test = vectorizer.transform(test_X)

from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=3)  
clf.fit(vec_train, train_y)
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
predictions_train = clf.predict(vec_train)
predictions_test = clf.predict(vec_test)

print("\nTest Set:")
print(classification_report(test_y, predictions_test))
print("Test Set Accuracy:", accuracy_score(test_y, predictions_test))

conf_matrix_test = confusion_matrix(test_y, predictions_test)
conf_matrix_train = confusion_matrix(train_y, predictions_train)

import matplotlib.pyplot as plt
import seaborn as sns
# Plotting Confusion Matrix
plt.figure(figsize=(8, 12))

plt.subplot(2, 1, 1)
sns.heatmap(conf_matrix_train, annot=True, fmt='d', cmap='Blues')
plt.title('Train Set Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

plt.subplot(2, 1, 2)
sns.heatmap(conf_matrix_test, annot=True, fmt='d', cmap='Blues')
plt.title('Test Set Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

corr_matrix = balanced_dataset.corr()
plt.figure(figsize=(20, 10))
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, cbar=False)
plt.title('Correlation Matrix for Fake News Detection')
plt.show()

import matplotlib.pyplot as plt
plt.tight_layout()
plt.show()
corr_matrix = pd.DataFrame(vec_train.toarray(), columns=vectorizer.get_feature_names_out())
corr = corr_matrix.corr()
plt.figure(figsize=(20, 10))
sns.heatmap(corr, cmap='coolwarm', annot=True, cbar=False)
plt.title('Correlation Matrix for Decision Tree Model')
plt.show()
